{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zdzichumis/Object-tracking-code/blob/main/Object%20tracking%20broken.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBIOss0Pg9O7",
        "outputId": "b3f79027-8d6f-4752-9989-b1992eb1d1a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/setup.py': [Errno 2] No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!mkdir -p ByteTrack\n",
        "!python3 setup.py develop"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install cython; pip3 install 'git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DXjJ76chAcE",
        "outputId": "f12f64f1-9c7a-463c-9a9e-51da3c022a9f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cython in /usr/local/lib/python3.11/dist-packages (3.0.12)\n",
            "Collecting git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI\n",
            "  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-req-build-_n228467\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/cocodataset/cocoapi.git /tmp/pip-req-build-_n228467\n",
            "  Resolved https://github.com/cocodataset/cocoapi.git to commit 8c9bcc3cf640524c4c20a9c40e89cb6a2f2fa0e9\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.11/dist-packages (from pycocotools==2.0) (75.2.0)\n",
            "Requirement already satisfied: cython>=0.27.3 in /usr/local/lib/python3.11/dist-packages (from pycocotools==2.0) (3.0.12)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pycocotools==2.0) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools==2.0) (1.17.0)\n",
            "Building wheels for collected packages: pycocotools\n",
            "  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycocotools: filename=pycocotools-2.0-cp311-cp311-linux_x86_64.whl size=395986 sha256=3202d343bc63964e7c3b3dbb61706321070ae9b174888bde0e6411d0b1835083\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-yf72qryf/wheels/6d/69/75/358c50a37672dfda8d74ba3b30ec49fb75d52f7c081886d503\n",
            "Successfully built pycocotools\n",
            "Installing collected packages: pycocotools\n",
            "  Attempting uninstall: pycocotools\n",
            "    Found existing installation: pycocotools 2.0.8\n",
            "    Uninstalling pycocotools-2.0.8:\n",
            "      Successfully uninstalled pycocotools-2.0.8\n",
            "Successfully installed pycocotools-2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-Ii_Es5-WC1",
        "outputId": "02c48dc7-04dd-4748-8267-2e3ecf27edc9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lap\n",
            "  Downloading lap-0.5.12-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from lap) (2.0.2)\n",
            "Downloading lap-0.5.12-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lap\n",
            "Successfully installed lap-0.5.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install loguru"
      ],
      "metadata": {
        "id": "J1zUiJXLgMIM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfbd0e52-ad01-439a-f496-1aa839dd548c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting loguru\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Downloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/61.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: loguru\n",
            "Successfully installed loguru-0.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install thop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiak7RFNHhYs",
        "outputId": "bed960ab-295f-446e-d09f-d0145a805b02"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting thop\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from thop) (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->thop) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->thop) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->thop) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->thop)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->thop)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->thop)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->thop)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->thop)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->thop)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->thop)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->thop)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->thop)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->thop)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->thop) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->thop) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->thop) (3.0.2)\n",
            "Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, thop\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 thop-0.1.1.post2209072238\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cython_bbox"
      ],
      "metadata": {
        "id": "vlQPV5ymMR0-",
        "outputId": "2c192e72-df09-4044-e669-295156156a68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cython_bbox\n",
            "  Downloading cython_bbox-0.1.5.tar.gz (4.4 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.11/dist-packages (from cython_bbox) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from cython_bbox) (2.0.2)\n",
            "Building wheels for collected packages: cython_bbox\n",
            "  Building wheel for cython_bbox (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cython_bbox: filename=cython_bbox-0.1.5-cp311-cp311-linux_x86_64.whl size=105317 sha256=332b4f707bb0bbe7cf6eab15c75fc62e35a4f741f5955291b1070c604fac8dff\n",
            "  Stored in directory: /root/.cache/pip/wheels/98/1b/e6/602b7896bbe63de2bd981e3c322026ffaeb1e70ddeb3f38974\n",
            "Successfully built cython_bbox\n",
            "Installing collected packages: cython_bbox\n",
            "Successfully installed cython_bbox-0.1.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import deque\n",
        "import os.path as osp\n",
        "import copy\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import cv2\n",
        "import scipy\n",
        "import lap\n",
        "import time\n",
        "from collections import OrderedDict\n",
        "from collections import defaultdict\n",
        "from scipy.spatial.distance import cdist\n",
        "from cython_bbox import bbox_overlaps as bbox_ious\n",
        "np.float = float\n",
        "\n",
        "os.chdir('ByteTrack')"
      ],
      "metadata": {
        "id": "gdCkRLajjU8F"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AccelerationKalmanFilter(object):\n",
        "    \"\"\"\n",
        "    A Kalman filter with a constant acceleration model for tracking bounding boxes.\n",
        "\n",
        "    State (10D): x, y, a, h, vx, vy, va, vh, ax, ay\n",
        "    Measurement (4D): x, y, a, h\n",
        "    where (x,y) is center, a: aspect ratio, h: height,\n",
        "    v*: velocities, ax/ay: accelerations on spatial axes.\n",
        "\n",
        "    Motion: discrete constant acceleration in X/Y;\n",
        "    Measurement: linear, observing [x,y,a,h]\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dt=1.):\n",
        "        self.ndim = 4  # measurement dims: x,y,a,h\n",
        "        self.dt = dt\n",
        "        self.state_dim = self.ndim * 2 + 2  # 4 pos + 4 vel + 2 acc =10\n",
        "        self.meas_dim = self.ndim             # 4\n",
        "\n",
        "        # Weights for process noise\n",
        "        self._std_weight_position = 1. / 20\n",
        "        self._std_weight_velocity = 1. / 160\n",
        "        self._std_weight_acceleration = 1. / 800\n",
        "\n",
        "        # Build motion matrix F (10x10)\n",
        "        self.motion_mat = np.eye(self.state_dim)\n",
        "        # Position->velocity and velocity->acceleration only for x,y\n",
        "        # Indices: 0:x, 1:y, 2:a, 3:h\n",
        "        # Velocities at 4:7, accelerations at 8:9\n",
        "        # For each spatial axis\n",
        "        for i in [0, 1]:  # x and y\n",
        "            self.motion_mat[i, self.ndim + i] = dt              # x += vx*dt\n",
        "            self.motion_mat[i, self.ndim*2 + (i-0)] = 0.5*dt**2  # x += 0.5*ax*dt^2\n",
        "            self.motion_mat[self.ndim + i, self.ndim*2 + (i-0)] = dt  # vx += ax*dt\n",
        "        # aspect ratio and height: constant velocity (no acceleration)\n",
        "        for i in [2, 3]:  # a, h\n",
        "            self.motion_mat[i, self.ndim + i] = dt\n",
        "\n",
        "        # Build update matrix H (4x10)\n",
        "        self.update_mat = np.zeros((self.meas_dim, self.state_dim))\n",
        "        self.update_mat[:, :self.meas_dim] = np.eye(self.meas_dim)\n",
        "\n",
        "    def initiate(self, measurement):\n",
        "        \"\"\"\n",
        "        Create initial state from measurement [x,y,a,h].\n",
        "        Velocities and accelerations initialized to zero.\n",
        "        Returns mean (10D) and covariance (10x10).\n",
        "        \"\"\"\n",
        "        mean_pos = measurement\n",
        "        mean_vel = np.zeros_like(mean_pos)\n",
        "        mean_acc = np.zeros(2)  # only ax, ay\n",
        "        mean = np.r_[mean_pos, mean_vel, mean_acc]\n",
        "\n",
        "        # Standard deviations scaled by height h\n",
        "        h = measurement[3]\n",
        "        std_pos = [2 * self._std_weight_position * h] * self.ndim\n",
        "        std_vel = [10 * self._std_weight_velocity * h] * self.ndim\n",
        "        std_acc = [20 * self._std_weight_acceleration * h] * 2  # x,y only\n",
        "        std = np.array(std_pos + std_vel + std_acc)\n",
        "\n",
        "        covariance = np.diag(np.square(std))\n",
        "        return mean, covariance\n",
        "\n",
        "    def predict(self, mean, covariance):\n",
        "        \"\"\"Run prediction step of KF.\"\"\"\n",
        "        h = mean[3]\n",
        "        std_pos = [self._std_weight_position * h] * self.ndim\n",
        "        std_vel = [self._std_weight_velocity * h] * self.ndim\n",
        "        std_acc = [self._std_weight_acceleration * h] * 2\n",
        "        motion_std = np.array(std_pos + std_vel + std_acc)\n",
        "        motion_cov = np.diag(np.square(motion_std))\n",
        "\n",
        "        mean = self.motion_mat @ mean\n",
        "        covariance = self.motion_mat @ covariance @ self.motion_mat.T + motion_cov\n",
        "        return mean, covariance\n",
        "\n",
        "    def project(self, mean, covariance):\n",
        "        \"\"\"Project state into measurement space.\"\"\"\n",
        "        std = [self._std_weight_position * mean[3]] * 2 + [1e-1] + [self._std_weight_position * mean[3]]\n",
        "        innovation_cov = np.diag(np.square(std))\n",
        "\n",
        "        proj_mean = self.update_mat @ mean\n",
        "        proj_cov = self.update_mat @ covariance @ self.update_mat.T + innovation_cov\n",
        "        return proj_mean, proj_cov\n",
        "\n",
        "    def update(self, mean, covariance, measurement):\n",
        "        \"\"\"Correction step with measurement [x,y,a,h].\"\"\"\n",
        "        proj_mean, proj_cov = self.project(mean, covariance)\n",
        "        chol, lower = scipy.linalg.cho_factor(proj_cov, lower=True, check_finite=False)\n",
        "        kalman_gain = scipy.linalg.cho_solve((chol, lower), (covariance @ self.update_mat.T).T,\n",
        "                                            check_finite=False).T\n",
        "        innovation = measurement - proj_mean\n",
        "        new_mean = mean + kalman_gain @ innovation\n",
        "        new_cov = covariance - kalman_gain @ proj_cov @ kalman_gain.T\n",
        "        return new_mean, new_cov\n",
        "\n",
        "    def multi_predict(self, means, covariances):\n",
        "        \"\"\"Vectorized prediction for multiple tracks.\"\"\"\n",
        "        motion_covs = []\n",
        "        for mean in means:\n",
        "            h = mean[3]\n",
        "            stds = np.array([\n",
        "                *([self._std_weight_position * h] * self.ndim),\n",
        "                *([self._std_weight_velocity * h] * self.ndim),\n",
        "                *([self._std_weight_acceleration * h] * 2)\n",
        "            ])\n",
        "            motion_covs.append(np.diag(stds**2))\n",
        "        motion_covs = np.stack(motion_covs, axis=0)\n",
        "\n",
        "        means_pred = (self.motion_mat @ means.T).T\n",
        "        covs_pred = np.einsum('ij,njk,kl->nil', self.motion_mat, covariances, self.motion_mat.T)\n",
        "        covs_pred += motion_covs\n",
        "        return means_pred, covs_pred\n",
        "\n",
        "    def gating_distance(self, mean, covariance, measurements,\n",
        "                        only_position=False, metric='maha'):\n",
        "        \"\"\"Compute Mahalanobis or Gaussian distance.\"\"\"\n",
        "        proj_mean, proj_cov = self.project(mean, covariance)\n",
        "        if only_position:\n",
        "            proj_mean = proj_mean[:2]\n",
        "            proj_cov = proj_cov[:2, :2]\n",
        "            measurements = measurements[:, :2]\n",
        "\n",
        "        d = measurements - proj_mean\n",
        "        if metric == 'gaussian':\n",
        "            return np.sum(d * d, axis=1)\n",
        "        elif metric == 'maha':\n",
        "            chol = np.linalg.cholesky(proj_cov)\n",
        "            z = scipy.linalg.solve_triangular(chol, d.T, lower=True)\n",
        "            return np.sum(z * z, axis=0)\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid metric {metric}\")"
      ],
      "metadata": {
        "id": "TPYPa-MScrap"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class KalmanFilter(object):\n",
        "    \"\"\"\n",
        "    A simple Kalman filter for tracking bounding boxes in image space.\n",
        "\n",
        "    The 8-dimensional state space\n",
        "\n",
        "        x, y, a, h, vx, vy, va, vh\n",
        "        where (x, y) is the center position, a the aspect ratio, h the height,\n",
        "        and (vx, vy) the corresponding velocities, and (va, vh) their\n",
        "        accelerations\n",
        "\n",
        "    Object motion follows a constant velocity model. The bounding box location\n",
        "    (x, y, a, h) is taken as direct observation of the state space (linear\n",
        "    observation model).\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        ndim, dt = 4, 1.\n",
        "\n",
        "        # Create Kalman filter model matrices.\n",
        "        self._motion_mat = np.eye(2 * ndim, 2 * ndim)\n",
        "        for i in range(ndim):\n",
        "            self._motion_mat[i, ndim + i] = dt\n",
        "        self._update_mat = np.eye(ndim, 2 * ndim)\n",
        "\n",
        "        # Motion and observation uncertainty are chosen relative to the current\n",
        "        # state estimate. These weights control the amount of uncertainty in\n",
        "        # the model. This is a bit hacky.\n",
        "        self._std_weight_position = 1. / 20\n",
        "        self._std_weight_velocity = 1. / 160\n",
        "\n",
        "    def initiate(self, measurement):\n",
        "        \"\"\"Create track from unassociated measurement.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        measurement : ndarray\n",
        "            Bounding box coordinates (x, y, a, h) with center position (x, y),\n",
        "            aspect ratio a, and height h.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        (ndarray, ndarray)\n",
        "            Returns the mean vector (8 dimensional) and covariance matrix (8x8\n",
        "            dimensional) of the new track. Unobserved velocities are initialized\n",
        "            to 0 mean.\n",
        "\n",
        "        \"\"\"\n",
        "        mean_pos = measurement\n",
        "        mean_vel = np.zeros_like(mean_pos)\n",
        "        mean = np.r_[mean_pos, mean_vel]\n",
        "\n",
        "        std = [\n",
        "            2 * self._std_weight_position * measurement[3],\n",
        "            2 * self._std_weight_position * measurement[3],\n",
        "            1e-2,\n",
        "            2 * self._std_weight_position * measurement[3],\n",
        "            10 * self._std_weight_velocity * measurement[3],\n",
        "            10 * self._std_weight_velocity * measurement[3],\n",
        "            1e-5,\n",
        "            10 * self._std_weight_velocity * measurement[3]]\n",
        "        covariance = np.diag(np.square(std))\n",
        "        return mean, covariance\n",
        "\n",
        "    def predict(self, mean, covariance):\n",
        "        \"\"\"Run Kalman filter prediction step.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        mean : ndarray\n",
        "            The 8 dimensional mean vector of the object state at the previous\n",
        "            time step.\n",
        "        covariance : ndarray\n",
        "            The 8x8 dimensional covariance matrix of the object state at the\n",
        "            previous time step.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        (ndarray, ndarray)\n",
        "            Returns the mean vector and covariance matrix of the predicted\n",
        "            state. Unobserved velocities are initialized to 0 mean.\n",
        "\n",
        "        \"\"\"\n",
        "        std_pos = [\n",
        "            self._std_weight_position * mean[3],\n",
        "            self._std_weight_position * mean[3],\n",
        "            1e-2,\n",
        "            self._std_weight_position * mean[3]]\n",
        "        std_vel = [\n",
        "            self._std_weight_velocity * mean[3],\n",
        "            self._std_weight_velocity * mean[3],\n",
        "            1e-5,\n",
        "            self._std_weight_velocity * mean[3]]\n",
        "        motion_cov = np.diag(np.square(np.r_[std_pos, std_vel]))\n",
        "\n",
        "        #mean = np.dot(self._motion_mat, mean)\n",
        "        mean = np.dot(mean, self._motion_mat.T)\n",
        "        covariance = np.linalg.multi_dot((\n",
        "            self._motion_mat, covariance, self._motion_mat.T)) + motion_cov\n",
        "\n",
        "        return mean, covariance\n",
        "\n",
        "    def project(self, mean, covariance):\n",
        "        \"\"\"Project state distribution to measurement space.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        mean : ndarray\n",
        "            The state's mean vector (8 dimensional array).\n",
        "        covariance : ndarray\n",
        "            The state's covariance matrix (8x8 dimensional).\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        (ndarray, ndarray)\n",
        "            Returns the projected mean and covariance matrix of the given state\n",
        "            estimate.\n",
        "\n",
        "        \"\"\"\n",
        "        std = [\n",
        "            self._std_weight_position * mean[3],\n",
        "            self._std_weight_position * mean[3],\n",
        "            1e-1,\n",
        "            self._std_weight_position * mean[3]]\n",
        "        innovation_cov = np.diag(np.square(std))\n",
        "\n",
        "        mean = np.dot(self._update_mat, mean)\n",
        "        covariance = np.linalg.multi_dot((\n",
        "            self._update_mat, covariance, self._update_mat.T))\n",
        "        return mean, covariance + innovation_cov\n",
        "\n",
        "    def multi_predict(self, mean, covariance):\n",
        "        \"\"\"Run Kalman filter prediction step (Vectorized version).\n",
        "        Parameters\n",
        "        ----------\n",
        "        mean : ndarray\n",
        "            The Nx8 dimensional mean matrix of the object states at the previous\n",
        "            time step.\n",
        "        covariance : ndarray\n",
        "            The Nx8x8 dimensional covariance matrics of the object states at the\n",
        "            previous time step.\n",
        "        Returns\n",
        "        -------\n",
        "        (ndarray, ndarray)\n",
        "            Returns the mean vector and covariance matrix of the predicted\n",
        "            state. Unobserved velocities are initialized to 0 mean.\n",
        "        \"\"\"\n",
        "        std_pos = [\n",
        "            self._std_weight_position * mean[:, 3],\n",
        "            self._std_weight_position * mean[:, 3],\n",
        "            1e-2 * np.ones_like(mean[:, 3]),\n",
        "            self._std_weight_position * mean[:, 3]]\n",
        "        std_vel = [\n",
        "            self._std_weight_velocity * mean[:, 3],\n",
        "            self._std_weight_velocity * mean[:, 3],\n",
        "            1e-5 * np.ones_like(mean[:, 3]),\n",
        "            self._std_weight_velocity * mean[:, 3]]\n",
        "        sqr = np.square(np.r_[std_pos, std_vel]).T\n",
        "\n",
        "        motion_cov = []\n",
        "        for i in range(len(mean)):\n",
        "            motion_cov.append(np.diag(sqr[i]))\n",
        "        motion_cov = np.asarray(motion_cov)\n",
        "\n",
        "        mean = np.dot(mean, self._motion_mat.T)\n",
        "        left = np.dot(self._motion_mat, covariance).transpose((1, 0, 2))\n",
        "        covariance = np.dot(left, self._motion_mat.T) + motion_cov\n",
        "\n",
        "        return mean, covariance\n",
        "\n",
        "    def update(self, mean, covariance, measurement):\n",
        "        \"\"\"Run Kalman filter correction step.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        mean : ndarray\n",
        "            The predicted state's mean vector (8 dimensional).\n",
        "        covariance : ndarray\n",
        "            The state's covariance matrix (8x8 dimensional).\n",
        "        measurement : ndarray\n",
        "            The 4 dimensional measurement vector (x, y, a, h), where (x, y)\n",
        "            is the center position, a the aspect ratio, and h the height of the\n",
        "            bounding box.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        (ndarray, ndarray)\n",
        "            Returns the measurement-corrected state distribution.\n",
        "\n",
        "        \"\"\"\n",
        "        projected_mean, projected_cov = self.project(mean, covariance)\n",
        "\n",
        "        chol_factor, lower = scipy.linalg.cho_factor(\n",
        "            projected_cov, lower=True, check_finite=False)\n",
        "        kalman_gain = scipy.linalg.cho_solve(\n",
        "            (chol_factor, lower), np.dot(covariance, self._update_mat.T).T,\n",
        "            check_finite=False).T\n",
        "        innovation = measurement - projected_mean\n",
        "\n",
        "        new_mean = mean + np.dot(innovation, kalman_gain.T)\n",
        "        new_covariance = covariance - np.linalg.multi_dot((\n",
        "            kalman_gain, projected_cov, kalman_gain.T))\n",
        "        return new_mean, new_covariance\n",
        "\n",
        "    def gating_distance(self, mean, covariance, measurements,\n",
        "                        only_position=False, metric='maha'):\n",
        "        \"\"\"Compute gating distance between state distribution and measurements.\n",
        "        A suitable distance threshold can be obtained from `chi2inv95`. If\n",
        "        `only_position` is False, the chi-square distribution has 4 degrees of\n",
        "        freedom, otherwise 2.\n",
        "        Parameters\n",
        "        ----------\n",
        "        mean : ndarray\n",
        "            Mean vector over the state distribution (8 dimensional).\n",
        "        covariance : ndarray\n",
        "            Covariance of the state distribution (8x8 dimensional).\n",
        "        measurements : ndarray\n",
        "            An Nx4 dimensional matrix of N measurements, each in\n",
        "            format (x, y, a, h) where (x, y) is the bounding box center\n",
        "            position, a the aspect ratio, and h the height.\n",
        "        only_position : Optional[bool]\n",
        "            If True, distance computation is done with respect to the bounding\n",
        "            box center position only.\n",
        "        Returns\n",
        "        -------\n",
        "        ndarray\n",
        "            Returns an array of length N, where the i-th element contains the\n",
        "            squared Mahalanobis distance between (mean, covariance) and\n",
        "            `measurements[i]`.\n",
        "        \"\"\"\n",
        "        mean, covariance = self.project(mean, covariance)\n",
        "        if only_position:\n",
        "            mean, covariance = mean[:2], covariance[:2, :2]\n",
        "            measurements = measurements[:, :2]\n",
        "\n",
        "        d = measurements - mean\n",
        "        if metric == 'gaussian':\n",
        "            return np.sum(d * d, axis=1)\n",
        "        elif metric == 'maha':\n",
        "            cholesky_factor = np.linalg.cholesky(covariance)\n",
        "            z = scipy.linalg.solve_triangular(\n",
        "                cholesky_factor, d.T, lower=True, check_finite=False,\n",
        "                overwrite_b=True)\n",
        "            squared_maha = np.sum(z * z, axis=0)\n",
        "            return squared_maha\n",
        "        else:\n",
        "            raise ValueError('invalid distance metric')"
      ],
      "metadata": {
        "id": "63B5eU4L_cFr"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Kalman_filter_used = KalmanFilter()"
      ],
      "metadata": {
        "id": "MbODXzHr99f6"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrackState(object):\n",
        "    New = 0\n",
        "    Tracked = 0\n",
        "    Lost = 0\n",
        "    Removed = 0\n",
        "\n",
        "\n",
        "class BaseTrack(object):\n",
        "    _count = 0\n",
        "\n",
        "    track_id = 0\n",
        "    is_activated = False\n",
        "    state = TrackState.New\n",
        "\n",
        "    history = OrderedDict()\n",
        "    features = []\n",
        "    curr_feature = None\n",
        "    score = 0\n",
        "    start_frame = 0\n",
        "    frame_id = 0\n",
        "    time_since_update = 0\n",
        "\n",
        "    # multi-camera\n",
        "    location = (np.inf, np.inf)\n",
        "\n",
        "    @property\n",
        "    def end_frame(self):\n",
        "        return self.frame_id\n",
        "\n",
        "    @staticmethod\n",
        "    def next_id():\n",
        "        BaseTrack._count += 1\n",
        "        return BaseTrack._count\n",
        "\n",
        "    def activate(self, *args):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def predict(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def update(self, *args, **kwargs):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def mark_lost(self):\n",
        "        self.state = TrackState.Lost\n",
        "\n",
        "    def mark_removed(self):\n",
        "        self.state = TrackState.Removed"
      ],
      "metadata": {
        "id": "HyjuiHQLBuZm"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Table for the 0.95 quantile of the chi-square distribution with N degrees of\n",
        "freedom (contains values for N=1, ..., 9). Taken from MATLAB/Octave's chi2inv\n",
        "function and used as Mahalanobis gating threshold.\n",
        "\"\"\"\n",
        "chi2inv95 = {\n",
        "    1: 3.8415,\n",
        "    2: 5.9915,\n",
        "    3: 7.8147,\n",
        "    4: 9.4877,\n",
        "    5: 11.070,\n",
        "    6: 12.592,\n",
        "    7: 14.067,\n",
        "    8: 15.507,\n",
        "    9: 16.919}\n",
        "\n",
        "def merge_matches(m1, m2, shape):\n",
        "    O,P,Q = shape\n",
        "    m1 = np.asarray(m1)\n",
        "    m2 = np.asarray(m2)\n",
        "\n",
        "    M1 = scipy.sparse.coo_matrix((np.ones(len(m1)), (m1[:, 0], m1[:, 1])), shape=(O, P))\n",
        "    M2 = scipy.sparse.coo_matrix((np.ones(len(m2)), (m2[:, 0], m2[:, 1])), shape=(P, Q))\n",
        "\n",
        "    mask = M1*M2\n",
        "    match = mask.nonzero()\n",
        "    match = list(zip(match[0], match[1]))\n",
        "    unmatched_O = tuple(set(range(O)) - set([i for i, j in match]))\n",
        "    unmatched_Q = tuple(set(range(Q)) - set([j for i, j in match]))\n",
        "\n",
        "    return match, unmatched_O, unmatched_Q\n",
        "\n",
        "\n",
        "def _indices_to_matches(cost_matrix, indices, thresh):\n",
        "    matched_cost = cost_matrix[tuple(zip(*indices))]\n",
        "    matched_mask = (matched_cost <= thresh)\n",
        "\n",
        "    matches = indices[matched_mask]\n",
        "    unmatched_a = tuple(set(range(cost_matrix.shape[0])) - set(matches[:, 0]))\n",
        "    unmatched_b = tuple(set(range(cost_matrix.shape[1])) - set(matches[:, 1]))\n",
        "\n",
        "    return matches, unmatched_a, unmatched_b\n",
        "\n",
        "\n",
        "def linear_assignment(cost_matrix, thresh):\n",
        "    if cost_matrix.size == 0:\n",
        "        return np.empty((0, 2), dtype=int), tuple(range(cost_matrix.shape[0])), tuple(range(cost_matrix.shape[1]))\n",
        "    matches, unmatched_a, unmatched_b = [], [], []\n",
        "    cost, x, y = lap.lapjv(cost_matrix, extend_cost=True, cost_limit=thresh)\n",
        "    for ix, mx in enumerate(x):\n",
        "        if mx >= 0:\n",
        "            matches.append([ix, mx])\n",
        "    unmatched_a = np.where(x < 0)[0]\n",
        "    unmatched_b = np.where(y < 0)[0]\n",
        "    matches = np.asarray(matches)\n",
        "    return matches, unmatched_a, unmatched_b\n",
        "\n",
        "\n",
        "def ious(atlbrs, btlbrs):\n",
        "    \"\"\"\n",
        "    Compute cost based on IoU\n",
        "    :type atlbrs: list[tlbr] | np.ndarray\n",
        "    :type atlbrs: list[tlbr] | np.ndarray\n",
        "\n",
        "    :rtype ious np.ndarray\n",
        "    \"\"\"\n",
        "    ious = np.zeros((len(atlbrs), len(btlbrs)), dtype=np.float)\n",
        "    if ious.size == 0:\n",
        "        return ious\n",
        "\n",
        "    ious = bbox_ious(\n",
        "        np.ascontiguousarray(atlbrs, dtype=np.float),\n",
        "        np.ascontiguousarray(btlbrs, dtype=np.float)\n",
        "    )\n",
        "\n",
        "    return ious\n",
        "\n",
        "\n",
        "def iou_distance(atracks, btracks):\n",
        "    \"\"\"\n",
        "    Compute cost based on IoU\n",
        "    :type atracks: list[STrack]\n",
        "    :type btracks: list[STrack]\n",
        "\n",
        "    :rtype cost_matrix np.ndarray\n",
        "    \"\"\"\n",
        "\n",
        "    if (len(atracks)>0 and isinstance(atracks[0], np.ndarray)) or (len(btracks) > 0 and isinstance(btracks[0], np.ndarray)):\n",
        "        atlbrs = atracks\n",
        "        btlbrs = btracks\n",
        "    else:\n",
        "        atlbrs = [track.tlbr for track in atracks]\n",
        "        btlbrs = [track.tlbr for track in btracks]\n",
        "    _ious = ious(atlbrs, btlbrs)\n",
        "    cost_matrix = 1 - _ious\n",
        "\n",
        "    return cost_matrix\n",
        "\n",
        "def v_iou_distance(atracks, btracks):\n",
        "    \"\"\"\n",
        "    Compute cost based on IoU\n",
        "    :type atracks: list[STrack]\n",
        "    :type btracks: list[STrack]\n",
        "\n",
        "    :rtype cost_matrix np.ndarray\n",
        "    \"\"\"\n",
        "\n",
        "    if (len(atracks)>0 and isinstance(atracks[0], np.ndarray)) or (len(btracks) > 0 and isinstance(btracks[0], np.ndarray)):\n",
        "        atlbrs = atracks\n",
        "        btlbrs = btracks\n",
        "    else:\n",
        "        atlbrs = [track.tlwh_to_tlbr(track.pred_bbox) for track in atracks]\n",
        "        btlbrs = [track.tlwh_to_tlbr(track.pred_bbox) for track in btracks]\n",
        "    _ious = ious(atlbrs, btlbrs)\n",
        "    cost_matrix = 1 - _ious\n",
        "\n",
        "    return cost_matrix\n",
        "\n",
        "def embedding_distance(tracks, detections, metric='cosine'):\n",
        "    \"\"\"\n",
        "    :param tracks: list[STrack]\n",
        "    :param detections: list[BaseTrack]\n",
        "    :param metric:\n",
        "    :return: cost_matrix np.ndarray\n",
        "    \"\"\"\n",
        "\n",
        "    cost_matrix = np.zeros((len(tracks), len(detections)), dtype=np.float)\n",
        "    if cost_matrix.size == 0:\n",
        "        return cost_matrix\n",
        "    det_features = np.asarray([track.curr_feat for track in detections], dtype=np.float)\n",
        "    #for i, track in enumerate(tracks):\n",
        "        #cost_matrix[i, :] = np.maximum(0.0, cdist(track.smooth_feat.reshape(1,-1), det_features, metric))\n",
        "    track_features = np.asarray([track.smooth_feat for track in tracks], dtype=np.float)\n",
        "    cost_matrix = np.maximum(0.0, cdist(track_features, det_features, metric))  # Nomalized features\n",
        "    return cost_matrix\n",
        "\n",
        "\n",
        "def gate_cost_matrix(kf, cost_matrix, tracks, detections, only_position=False):\n",
        "    if cost_matrix.size == 0:\n",
        "        return cost_matrix\n",
        "    gating_dim = 2 if only_position else 4\n",
        "    gating_threshold = chi2inv95[gating_dim]\n",
        "    measurements = np.asarray([det.to_xyah() for det in detections])\n",
        "    for row, track in enumerate(tracks):\n",
        "        gating_distance = kf.gating_distance(\n",
        "            track.mean, track.covariance, measurements, only_position)\n",
        "        cost_matrix[row, gating_distance > gating_threshold] = np.inf\n",
        "    return cost_matrix\n",
        "\n",
        "\n",
        "def fuse_motion(kf, cost_matrix, tracks, detections, only_position=False, lambda_=0.98):\n",
        "    if cost_matrix.size == 0:\n",
        "        return cost_matrix\n",
        "    gating_dim = 2 if only_position else 4\n",
        "    gating_threshold = chi2inv95[gating_dim]\n",
        "    measurements = np.asarray([det.to_xyah() for det in detections])\n",
        "    for row, track in enumerate(tracks):\n",
        "        gating_distance = kf.gating_distance(\n",
        "            track.mean, track.covariance, measurements, only_position, metric='maha')\n",
        "        cost_matrix[row, gating_distance > gating_threshold] = np.inf\n",
        "        cost_matrix[row] = lambda_ * cost_matrix[row] + (1 - lambda_) * gating_distance\n",
        "    return cost_matrix\n",
        "\n",
        "\n",
        "def fuse_iou(cost_matrix, tracks, detections):\n",
        "    if cost_matrix.size == 0:\n",
        "        return cost_matrix\n",
        "    reid_sim = 1 - cost_matrix\n",
        "    iou_dist = iou_distance(tracks, detections)\n",
        "    iou_sim = 1 - iou_dist\n",
        "    fuse_sim = reid_sim * (1 + iou_sim) / 2\n",
        "    det_scores = np.array([det.score for det in detections])\n",
        "    det_scores = np.expand_dims(det_scores, axis=0).repeat(cost_matrix.shape[0], axis=0)\n",
        "    #fuse_sim = fuse_sim * (1 + det_scores) / 2\n",
        "    fuse_cost = 1 - fuse_sim\n",
        "    return fuse_cost\n",
        "\n",
        "\n",
        "def fuse_score(cost_matrix, detections):\n",
        "    if cost_matrix.size == 0:\n",
        "        return cost_matrix\n",
        "    iou_sim = 1 - cost_matrix\n",
        "    det_scores = np.array([det.score for det in detections])\n",
        "    det_scores = np.expand_dims(det_scores, axis=0).repeat(cost_matrix.shape[0], axis=0)\n",
        "    fuse_sim = iou_sim * det_scores\n",
        "    fuse_cost = 1 - fuse_sim\n",
        "    return fuse_cost"
      ],
      "metadata": {
        "id": "UGsO9u6A_3bt"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class STrack(BaseTrack):\n",
        "    def __init__(self, tlwh, score):\n",
        "\n",
        "        # wait activate\n",
        "        self._tlwh = np.asarray(tlwh, dtype=np.float)\n",
        "        self.kalman_filter = None\n",
        "        self.mean, self.covariance = None, None\n",
        "        self.is_activated = False\n",
        "\n",
        "        self.score = score\n",
        "        self.tracklet_len = 0\n",
        "\n",
        "    def predict(self):\n",
        "        mean_state = self.mean.copy()\n",
        "        if self.state != TrackState.Tracked:\n",
        "            mean_state[7] = 0\n",
        "        self.mean, self.covariance = self.kalman_filter.predict(mean_state, self.covariance)\n",
        "\n",
        "    @staticmethod\n",
        "    def multi_predict(stracks):\n",
        "        if len(stracks) > 0:\n",
        "            multi_mean = np.asarray([st.mean.copy() for st in stracks])\n",
        "            multi_covariance = np.asarray([st.covariance for st in stracks])\n",
        "            for i, st in enumerate(stracks):\n",
        "                if st.state != TrackState.Tracked:\n",
        "                    multi_mean[i][7] = 0\n",
        "            multi_mean, multi_covariance = Kalman_filter_used.multi_predict(multi_mean, multi_covariance)\n",
        "            for i, (mean, cov) in enumerate(zip(multi_mean, multi_covariance)):\n",
        "                stracks[i].mean = mean\n",
        "                stracks[i].covariance = cov\n",
        "\n",
        "    def activate(self, kalman_filter, frame_id):\n",
        "        \"\"\"Start a new tracklet\"\"\"\n",
        "        self.kalman_filter = kalman_filter\n",
        "        self.track_id = self.next_id()\n",
        "        self.mean, self.covariance = self.kalman_filter.initiate(self.tlwh_to_xyah(self._tlwh))\n",
        "\n",
        "        self.tracklet_len = 0\n",
        "        self.state = TrackState.Tracked\n",
        "        if frame_id == 1:\n",
        "            self.is_activated = True\n",
        "        # self.is_activated = True\n",
        "        self.frame_id = frame_id\n",
        "        self.start_frame = frame_id\n",
        "\n",
        "    def re_activate(self, new_track, frame_id, new_id=False):\n",
        "        self.mean, self.covariance = self.kalman_filter.update(\n",
        "            self.mean, self.covariance, self.tlwh_to_xyah(new_track.tlwh)\n",
        "        )\n",
        "        self.tracklet_len = 0\n",
        "        self.state = TrackState.Tracked\n",
        "        self.is_activated = True\n",
        "        self.frame_id = frame_id\n",
        "        if new_id:\n",
        "            self.track_id = self.next_id()\n",
        "        self.score = new_track.score\n",
        "\n",
        "    def update(self, new_track, frame_id):\n",
        "        \"\"\"\n",
        "        Update a matched track\n",
        "        :type new_track: STrack\n",
        "        :type frame_id: int\n",
        "        :type update_feature: bool\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        self.frame_id = frame_id\n",
        "        self.tracklet_len += 1\n",
        "\n",
        "        new_tlwh = new_track.tlwh\n",
        "        self.mean, self.covariance = self.kalman_filter.update(\n",
        "            self.mean, self.covariance, self.tlwh_to_xyah(new_tlwh))\n",
        "        self.state = TrackState.Tracked\n",
        "        self.is_activated = True\n",
        "\n",
        "        self.score = new_track.score\n",
        "\n",
        "    @property\n",
        "    # @jit(nopython=True)\n",
        "    def tlwh(self):\n",
        "        \"\"\"Get current position in bounding box format `(top left x, top left y,\n",
        "                width, height)`.\n",
        "        \"\"\"\n",
        "        if self.mean is None:\n",
        "            return self._tlwh.copy()\n",
        "        ret = self.mean[:4].copy()\n",
        "        ret[2] *= ret[3]\n",
        "        ret[:2] -= ret[2:] / 2\n",
        "        return ret\n",
        "\n",
        "    @property\n",
        "    # @jit(nopython=True)\n",
        "    def tlbr(self):\n",
        "        \"\"\"Convert bounding box to format `(min x, min y, max x, max y)`, i.e.,\n",
        "        `(top left, bottom right)`.\n",
        "        \"\"\"\n",
        "        ret = self.tlwh.copy()\n",
        "        ret[2:] += ret[:2]\n",
        "        return ret\n",
        "\n",
        "    @staticmethod\n",
        "    # @jit(nopython=True)\n",
        "    def tlwh_to_xyah(tlwh):\n",
        "        \"\"\"Convert bounding box to format `(center x, center y, aspect ratio,\n",
        "        height)`, where the aspect ratio is `width / height`.\n",
        "        \"\"\"\n",
        "        ret = np.asarray(tlwh).copy()\n",
        "        ret[:2] += ret[2:] / 2\n",
        "        ret[2] /= ret[3]\n",
        "        return ret\n",
        "\n",
        "    def to_xyah(self):\n",
        "        return self.tlwh_to_xyah(self.tlwh)\n",
        "\n",
        "    @staticmethod\n",
        "    # @jit(nopython=True)\n",
        "    def tlbr_to_tlwh(tlbr):\n",
        "        ret = np.asarray(tlbr).copy()\n",
        "        ret[2:] -= ret[:2]\n",
        "        return ret\n",
        "\n",
        "    @staticmethod\n",
        "    # @jit(nopython=True)\n",
        "    def tlwh_to_tlbr(tlwh):\n",
        "        ret = np.asarray(tlwh).copy()\n",
        "        ret[2:] += ret[:2]\n",
        "        return ret\n",
        "\n",
        "    def __repr__(self):\n",
        "        return 'OT_{}_({}-{})'.format(self.track_id, self.start_frame, self.end_frame)\n",
        "\n",
        "\n",
        "class BYTETracker(object):\n",
        "    def __init__(self, args, frame_rate=30):\n",
        "        self.tracked_stracks = []  # type: list[STrack]\n",
        "        self.lost_stracks = []  # type: list[STrack]\n",
        "        self.removed_stracks = []  # type: list[STrack]\n",
        "\n",
        "        self.frame_id = 0\n",
        "        self.args = args\n",
        "        #self.det_thresh = args.track_thresh\n",
        "        self.det_thresh = args.track_thresh + 0.1\n",
        "        self.buffer_size = int(frame_rate / 30.0 * args.track_buffer)\n",
        "        self.max_time_lost = self.buffer_size\n",
        "        self.kalman_filter = Kalman_filter_used\n",
        "\n",
        "    def update(self, output_results, img_info, img_size):\n",
        "        self.frame_id += 1\n",
        "        activated_starcks = []\n",
        "        refind_stracks = []\n",
        "        lost_stracks = []\n",
        "        removed_stracks = []\n",
        "\n",
        "        if output_results.shape[1] == 5:\n",
        "            scores = output_results[:, 4]\n",
        "            bboxes = output_results[:, :4]\n",
        "        else:\n",
        "            output_results = output_results.cpu().numpy()\n",
        "            scores = output_results[:, 4] * output_results[:, 5]\n",
        "            bboxes = output_results[:, :4]  # x1y1x2y2\n",
        "        img_h, img_w = img_info[0], img_info[1]\n",
        "        scale = min(img_size[0] / float(img_h), img_size[1] / float(img_w))\n",
        "        bboxes /= scale\n",
        "\n",
        "        remain_inds = scores > self.args.track_thresh\n",
        "        inds_low = scores > 0.1\n",
        "        inds_high = scores < self.args.track_thresh\n",
        "\n",
        "        inds_second = np.logical_and(inds_low, inds_high)\n",
        "        dets_second = bboxes[inds_second]\n",
        "        dets = bboxes[remain_inds]\n",
        "        scores_keep = scores[remain_inds]\n",
        "        scores_second = scores[inds_second]\n",
        "\n",
        "        if len(dets) > 0:\n",
        "            '''Detections'''\n",
        "            detections = [STrack(STrack.tlbr_to_tlwh(tlbr), s) for\n",
        "                          (tlbr, s) in zip(dets, scores_keep)]\n",
        "        else:\n",
        "            detections = []\n",
        "\n",
        "        ''' Add newly detected tracklets to tracked_stracks'''\n",
        "        unconfirmed = []\n",
        "        tracked_stracks = []  # type: list[STrack]\n",
        "        for track in self.tracked_stracks:\n",
        "            if not track.is_activated:\n",
        "                unconfirmed.append(track)\n",
        "            else:\n",
        "                tracked_stracks.append(track)\n",
        "\n",
        "        ''' Step 2: First association, with high score detection boxes'''\n",
        "        strack_pool = joint_stracks(tracked_stracks, self.lost_stracks)\n",
        "        # Predict the current location with KF\n",
        "        STrack.multi_predict(strack_pool)\n",
        "        dists = iou_distance(strack_pool, detections)\n",
        "        if not self.args.mot20:\n",
        "            dists = fuse_score(dists, detections)\n",
        "        matches, u_track, u_detection = linear_assignment(dists, thresh=self.args.match_thresh)\n",
        "\n",
        "        for itracked, idet in matches:\n",
        "            track = strack_pool[itracked]\n",
        "            det = detections[idet]\n",
        "            if track.state == TrackState.Tracked:\n",
        "                track.update(detections[idet], self.frame_id)\n",
        "                activated_starcks.append(track)\n",
        "            else:\n",
        "                track.re_activate(det, self.frame_id, new_id=False)\n",
        "                refind_stracks.append(track)\n",
        "\n",
        "        ''' Step 3: Second association, with low score detection boxes'''\n",
        "        # association the untrack to the low score detections\n",
        "        if len(dets_second) > 0:\n",
        "            '''Detections'''\n",
        "            detections_second = [STrack(STrack.tlbr_to_tlwh(tlbr), s) for\n",
        "                          (tlbr, s) in zip(dets_second, scores_second)]\n",
        "        else:\n",
        "            detections_second = []\n",
        "        r_tracked_stracks = [strack_pool[i] for i in u_track if strack_pool[i].state == TrackState.Tracked]\n",
        "        dists = iou_distance(r_tracked_stracks, detections_second)\n",
        "        matches, u_track, u_detection_second = linear_assignment(dists, thresh=0.5)\n",
        "        for itracked, idet in matches:\n",
        "            track = r_tracked_stracks[itracked]\n",
        "            det = detections_second[idet]\n",
        "            if track.state == TrackState.Tracked:\n",
        "                track.update(det, self.frame_id)\n",
        "                activated_starcks.append(track)\n",
        "            else:\n",
        "                track.re_activate(det, self.frame_id, new_id=False)\n",
        "                refind_stracks.append(track)\n",
        "\n",
        "        for it in u_track:\n",
        "            track = r_tracked_stracks[it]\n",
        "            if not track.state == TrackState.Lost:\n",
        "                track.mark_lost()\n",
        "                lost_stracks.append(track)\n",
        "\n",
        "        '''Deal with unconfirmed tracks, usually tracks with only one beginning frame'''\n",
        "        detections = [detections[i] for i in u_detection]\n",
        "        dists = iou_distance(unconfirmed, detections)\n",
        "        if not self.args.mot20:\n",
        "            dists = fuse_score(dists, detections)\n",
        "        matches, u_unconfirmed, u_detection = linear_assignment(dists, thresh=0.7)\n",
        "        for itracked, idet in matches:\n",
        "            unconfirmed[itracked].update(detections[idet], self.frame_id)\n",
        "            activated_starcks.append(unconfirmed[itracked])\n",
        "        for it in u_unconfirmed:\n",
        "            track = unconfirmed[it]\n",
        "            track.mark_removed()\n",
        "            removed_stracks.append(track)\n",
        "\n",
        "        \"\"\" Step 4: Init new stracks\"\"\"\n",
        "        for inew in u_detection:\n",
        "            track = detections[inew]\n",
        "            if track.score < self.det_thresh:\n",
        "                continue\n",
        "            track.activate(self.kalman_filter, self.frame_id)\n",
        "            activated_starcks.append(track)\n",
        "        \"\"\" Step 5: Update state\"\"\"\n",
        "        for track in self.lost_stracks:\n",
        "            if self.frame_id - track.end_frame > self.max_time_lost:\n",
        "                track.mark_removed()\n",
        "                removed_stracks.append(track)\n",
        "\n",
        "        self.tracked_stracks = [t for t in self.tracked_stracks if t.state == TrackState.Tracked]\n",
        "        self.tracked_stracks = joint_stracks(self.tracked_stracks, activated_starcks)\n",
        "        self.tracked_stracks = joint_stracks(self.tracked_stracks, refind_stracks)\n",
        "        self.lost_stracks = sub_stracks(self.lost_stracks, self.tracked_stracks)\n",
        "        self.lost_stracks.extend(lost_stracks)\n",
        "        self.lost_stracks = sub_stracks(self.lost_stracks, self.removed_stracks)\n",
        "        self.removed_stracks.extend(removed_stracks)\n",
        "        self.tracked_stracks, self.lost_stracks = remove_duplicate_stracks(self.tracked_stracks, self.lost_stracks)\n",
        "        # get scores of lost tracks\n",
        "        output_stracks = [track for track in self.tracked_stracks if track.is_activated]\n",
        "\n",
        "        return output_stracks\n",
        "\n",
        "\n",
        "def joint_stracks(tlista, tlistb):\n",
        "    exists = {}\n",
        "    res = []\n",
        "    for t in tlista:\n",
        "        exists[t.track_id] = 1\n",
        "        res.append(t)\n",
        "    for t in tlistb:\n",
        "        tid = t.track_id\n",
        "        if not exists.get(tid, 0):\n",
        "            exists[tid] = 1\n",
        "            res.append(t)\n",
        "    return res\n",
        "\n",
        "\n",
        "def sub_stracks(tlista, tlistb):\n",
        "    stracks = {}\n",
        "    for t in tlista:\n",
        "        stracks[t.track_id] = t\n",
        "    for t in tlistb:\n",
        "        tid = t.track_id\n",
        "        if stracks.get(tid, 0):\n",
        "            del stracks[tid]\n",
        "    return list(stracks.values())\n",
        "\n",
        "\n",
        "def remove_duplicate_stracks(stracksa, stracksb):\n",
        "    pdist = iou_distance(stracksa, stracksb)\n",
        "    pairs = np.where(pdist < 0.15)\n",
        "    dupa, dupb = list(), list()\n",
        "    for p, q in zip(*pairs):\n",
        "        timep = stracksa[p].frame_id - stracksa[p].start_frame\n",
        "        timeq = stracksb[q].frame_id - stracksb[q].start_frame\n",
        "        if timep > timeq:\n",
        "            dupb.append(q)\n",
        "        else:\n",
        "            dupa.append(p)\n",
        "    resa = [t for i, t in enumerate(stracksa) if not i in dupa]\n",
        "    resb = [t for i, t in enumerate(stracksb) if not i in dupb]\n",
        "    return resa, resb"
      ],
      "metadata": {
        "id": "iIXYbxbG-WUt"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ────────────────────────────────────────────────────────────────────────────────────────────────────\n",
        "# CONFIGURATION\n",
        "# ────────────────────────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "yolo_csv = \"yolo_pred_04_30_18h_18.csv\"\n",
        "track_csv  = \"track_04_30_18h_18.csv\"\n",
        "scale_rate = 0.01\n",
        "\n",
        "class Info:\n",
        "    def __init__(self):\n",
        "        # detection confidence threshold\n",
        "        self.track_thresh = 0.4\n",
        "        # how long to keep a \"lost\" track before deleting\n",
        "        self.track_buffer = 30\n",
        "        # matching IoU threshold\n",
        "        self.match_thresh = 0.7\n",
        "        # boxes below minimal area are ignored\n",
        "        self.min_box_area = 10\n",
        "        #not used yet\n",
        "        self.mot20 = False\n",
        "        #not used yet\n",
        "        self.mot20_non_mot17 = False\n",
        "\n",
        "tracker = BYTETracker(Info())\n",
        "frame_width, frame_height, fps = 1280, 720, 30\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────────────────────────\n",
        "# MAIN CODE: READ YOLO PREDICTIONS, UPDATE TRACKER, FILL MISSING FRAMES WITH KALMAN FILTER PREDICTIONS\n",
        "# ────────────────────────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "data = pd.read_csv(yolo_csv)\n",
        "\n",
        "print(\"Processed (min):\")\n",
        "miss_counts = defaultdict(int)\n",
        "\n",
        "with open(track_csv, \"w\") as f:\n",
        "    f.write(\"x0,y0,x1,y1,label,detected,frame\\n\")\n",
        "\n",
        "    for frame_idx, det_group in data.groupby(\"frame\", sort=True):\n",
        "        dets = det_group[[\"x0\",\"y0\",\"x1\",\"y1\",\"conf\"]].values\n",
        "\n",
        "        online_targets = tracker.update(dets, img_info=(frame_width, frame_height),\n",
        "                                        img_size=(frame_width, frame_height),)\n",
        "\n",
        "        lost_targets = [t for t in tracker.lost_stracks if t.state == TrackState.Lost]\n",
        "\n",
        "        for target in online_targets:\n",
        "            miss_counts[target.track_id] = 0\n",
        "\n",
        "        for target in lost_targets:\n",
        "            miss_counts[target.track_id] += 1\n",
        "\n",
        "        all_tracks = online_targets + lost_targets\n",
        "\n",
        "        for trk in all_tracks:\n",
        "            x0, y0, x1, y1 = map(float, trk.tlbr)\n",
        "            is_detected = int(trk not in lost_targets)\n",
        "\n",
        "            if not is_detected:\n",
        "                m = miss_counts[trk.track_id]\n",
        "                s = 1.0 + scale_rate * m\n",
        "\n",
        "                cx, cy = (x0 + x1) / 2, (y0 + y1) / 2\n",
        "                w, h   = (x1 - x0) * s, (y1 - y0) * s\n",
        "\n",
        "                x0 = cx - w/2\n",
        "                y0 = cy - h/2\n",
        "                x1 = cx + w/2\n",
        "                y1 = cy + h/2\n",
        "\n",
        "                x0 = max(0, x0); y0 = max(0, y0)\n",
        "                x1 = min(frame_width, x1); y1 = min(frame_height, y1)\n",
        "            f.write(f\"{int(x0)},{int(y0)},{int(x1)},{int(y1)},{trk.track_id},{is_detected},{frame_idx}\\n\")\n",
        "\n",
        "        if frame_idx % (fps * 60) == 0 and frame_idx > 0:\n",
        "            minutes = frame_idx // fps // 60\n",
        "            print(f\"{minutes}\", end=\" \")\n",
        "        if frame_idx > fps*120:\n",
        "            break\n",
        "print(\"Tracking saved:\", track_csv)"
      ],
      "metadata": {
        "id": "k1rvWLZLOC8t",
        "outputId": "0f03c721-651a-4579-d457-6e14307de95f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed (min):\n",
            "1 2 Tracking saved: track_04_30_18h_18.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ────────────────────────────────────────────────────────────────────────────────────────────────────\n",
        "# FILE NAMES\n",
        "# ────────────────────────────────────────────────────────────────────────────────────────────────────\n",
        "output_file = 'tracked_04_30_18_18.mp4'\n",
        "input_video = 'output_04_30_18h_18.mp4'\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────────────────────────────\n",
        "# VIDEO CREATION\n",
        "# ────────────────────────────────────────────────────────────────────────────────────────────────────\n",
        "\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "video_writer = cv2.VideoWriter(output_file, fourcc, fps, (frame_width, frame_height))\n",
        "seen = dict()\n",
        "cap = cv2.VideoCapture(input_video)\n",
        "if not cap.isOpened():\n",
        "    raise IOError(f\"Cannot open video file {input_video}\")\n",
        "df = pd.read_csv(track_csv)\n",
        "i = 0\n",
        "print(\"Processed (min):\")\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        print(\"Can't receive frame (stream end?). Exiting ...\")\n",
        "        break\n",
        "\n",
        "    # Filter the DataFrame for current frame\n",
        "    frame_data = df[df['frame'] == i]\n",
        "\n",
        "    for _, row in frame_data.iterrows():\n",
        "        x0, y0, x1, y1 = int(row['x0']), int(row['y0']), int(row['x1']), int(row['y1'])\n",
        "        label = str(row['label'])\n",
        "        detected = int(row['detected'])\n",
        "        colour = (0, 255, 0)\n",
        "        if detected == 0:\n",
        "            colour = (0, 255, 255)\n",
        "        else:\n",
        "            if label not in seen:\n",
        "                seen[label] = 0\n",
        "            if seen[label] < fps/2:\n",
        "                colour = (127, 255, 127)\n",
        "        seen[label] += 1\n",
        "        cv2.rectangle(frame, (x0, y0), (x1, y1), colour, 2)\n",
        "        cv2.putText(frame, label, (x0, y0 - 10), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                    0.5, colour, 1, cv2.LINE_AA)\n",
        "    video_writer.write(frame)\n",
        "    i += 1\n",
        "    if i % (fps*60) == 0:\n",
        "        minutes = i // fps // 60\n",
        "        print(minutes, end = \" \")\n",
        "    if i > fps*120:\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "video_writer.release()\n",
        "print(\"Video saved:\", output_file)"
      ],
      "metadata": {
        "id": "j2vkI8Zf2A8T",
        "outputId": "23b53a18-a599-4d47-ddfb-fe54144292fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed (min):\n",
            "1 2 Video saved: tracked_04_30_18_18.mp4\n"
          ]
        }
      ]
    }
  ]
}